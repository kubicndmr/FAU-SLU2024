{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kubicndmr/FAU-SLU2024/blob/main/Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JCgSa_iAZP7"
      },
      "source": [
        "# **Exercise 3:**\n",
        "\n",
        "# **Word and Sentence Embeddings**\n",
        "---\n",
        "The context has an important role in the meaning of words. Words that occur in similar contexts tend to have similar meanings. In this exercise, we will see techniques for learning representations of the meanings of words. Then, we will extend this idea to sentences and perform a semantic analysis task by using machine learning classfiers.\n",
        "\n",
        "\n",
        "### **Acknowledgement**\n",
        "Some code snippets are retrieved from Standford CS224N material. See their website for more details.\n",
        "\n",
        "You can also see these resources for further reading.\n",
        "\n",
        "[1] Jurafsky, Daniel, and James H. Martin. \"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StWuU_ixUVNm"
      },
      "source": [
        "## **3.1 Word Vectors**\n",
        "\n",
        "Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n",
        "Note that word vectors and word embeddings are often used interchangeably. They refer to the mathematical representation of the words as vectors in the feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpAiPL7rWhac"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First, install the necessary packages that do not exist in the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rUCI7y-rWWbl"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSAsvn0sT0bP"
      },
      "source": [
        "Then, import necessary packages:\n",
        "\n",
        "*   NumPy is an open source project that enables numerical computing with Python\n",
        "*   sklearn is an open source machine learning library\n",
        "*   matplotlib is a widely used Python package for creating visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8hWBgAR8Y3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fLiYaEPYhbk"
      },
      "outputs": [],
      "source": [
        "# define parameters\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "NUM_SAMPLES = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrWUJN5SlZVb"
      },
      "source": [
        "Let's get some insights from the dataset we will be working on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_tw4ep6YdQp"
      },
      "outputs": [],
      "source": [
        "# load the imbd dataset\n",
        "sentiment_dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
        "\n",
        "# get subset only\n",
        "data = sentiment_dataset[\"train\"][:NUM_SAMPLES]\n",
        "\n",
        "# see how data classes look like\n",
        "print(data.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNSWb9bimEoa"
      },
      "source": [
        "The dataset has three classes: positive, negative, and neutral. Our task is classifiying tweets given in the text column to these classes. Examples from these classes are given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwPu_idiYfym"
      },
      "outputs": [],
      "source": [
        "def print_example(data, idx):\n",
        "  print('\\nID:\\t\\t', data['id'][idx])\n",
        "  print('Text:\\t\\t', data['text'][idx])\n",
        "  print('Label:\\t\\t',data['label'][idx])\n",
        "  print('Label_Text:\\t',data['label_text'][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnVNrBqPWSuo"
      },
      "outputs": [],
      "source": [
        "# Neutral example\n",
        "print_example(data, 10)\n",
        "\n",
        "# Positive Example\n",
        "print_example(data, 11)\n",
        "\n",
        "# Negative Example\n",
        "print_example(data, 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5LXjCdoOI8"
      },
      "source": [
        "### **Question:** How can we convert the text into numbers/vectors/tensors such that we can process them mathematically?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M4nRia8ttuw"
      },
      "source": [
        "### **3.1.1 One-Hot Vector**\n",
        "\n",
        "A straightforward method is to create a one-hot vector and represent each word with a unique array of numbers. We can do it in two steps:\n",
        "\n",
        "  * Start with finding a dictionary size of the dataset, i.e., number of unique words.\n",
        "\n",
        "  * After finding the dictionary size, create a one-hot coded vector for each word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDQ-QTxGvYEt"
      },
      "source": [
        "▶*TODO* Complete `dictionary_size` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpfnvpNjoM3X"
      },
      "outputs": [],
      "source": [
        "def dictionary_size(data):\n",
        "  '''\n",
        "  This function will return number of unique words in the given dataset.\n",
        "  For simplicity, consider each word as th array of strings between two\n",
        "  spaces. Count words written with combinations as new words.\n",
        "  For example 'I'm' and 'I', 'am' are three different words.\n",
        "\n",
        "  Params:\n",
        "        data (dictionary): corpus of documents\n",
        "  Return:\n",
        "        dict_size (integer): vocabulary size of the given dataset\n",
        "\n",
        "  '''\n",
        "  unique_words = set()\n",
        "  text_data = data['text']\n",
        "\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "\n",
        "\n",
        "  # ------------------\n",
        "\n",
        "  return len(unique_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mtWxvl5nxU7U"
      },
      "outputs": [],
      "source": [
        "# Some sanity check (note that this is not an exhaustive test)\n",
        "assert (dictionary_size(sentiment_dataset[\"train\"][:2]) == 16), f'The correct count is 16, yours is {dictionary_size(sentiment_dataset[\"train\"][:2])}'\n",
        "assert (dictionary_size(sentiment_dataset[\"train\"][:10]) == 74), f'The correct count is 74, yours is {dictionary_size(sentiment_dataset[\"train\"][:10])}'\n",
        "\n",
        "print(\"All good!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev6r7Dduvoke"
      },
      "outputs": [],
      "source": [
        "# get the size of dataset\n",
        "dataset_size = dictionary_size(data)\n",
        "print(f'The number of uniq words in the dataset is: {dataset_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_aDX4d21Usx"
      },
      "source": [
        "Now, lets observe the vocabulary size of the dataset as it grows. Initially, we set `NUM_SAMPLES` to 50. Change it iteratively and see how it grows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-fHHI2S1TyF"
      },
      "outputs": [],
      "source": [
        "vocab_size = []\n",
        "num_sample_list = [15, 50, 150, 500, 1500, 5000, 15000, 50000, 150000]\n",
        "\n",
        "for num_samples in num_sample_list:\n",
        "  vocab_size.append(dictionary_size(sentiment_dataset[\"train\"][:num_samples]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wThmvPCH29ii"
      },
      "source": [
        "Plot it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG6FPD2m285N"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(vocab_size, num_sample_list, color='red', linewidth=2.5)\n",
        "plt.xlabel('Dataset Size (Required Vector Length)')\n",
        "plt.ylabel('Vocabulary Size')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVizsHSF3-zR"
      },
      "source": [
        "#### **There are two problems**\n",
        "\n",
        "As you can see, this is not a feasible approach. The number of unique vectors are growing exponentially and even in a relatively small dataset, created one-hot vectors will occupy huge part of the memory or not fit in it at all. We need to map them into much lower dimensions.\n",
        "\n",
        "Moreover, we want to create word vectors such that they contain also semantic relationships. This information is **not** encoded in one-hot vectors. Ideally, we want something like this.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1-Y9WzaKR6SI3cYp_yp_RcDDPC9zK9I9A)\n",
        "\n",
        "(Image retrieved from: https://web.stanford.edu/~jurafsky/slp3/6.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT6okoUBDHEC"
      },
      "source": [
        "### **3.1.2 Co-Occurence Matirx**\n",
        "\n",
        "To respond to these two problems, we can use a co-occurrence matrix to integrate information on word relationships into word vectors and use dimensionality reduction techniques to keep vector size lower.\n",
        "\n",
        "A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the *context window* surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window among all documents.\n",
        "\n",
        "**Example: Co-Occurrence with Fixed Window of n=1**:\n",
        "\n",
        "Document 1: \"all that glitters is not gold\"\n",
        "\n",
        "Document 2: \"all is well that ends well\"\n",
        "\n",
        "\n",
        "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
        "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
        "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
        "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
        "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
        "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
        "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
        "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
        "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
        "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
        "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
        "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
        "\n",
        "In NLP, we commonly use `<START>` and `<END>` tokens to mark the beginning and end of sentences, paragraphs, or documents. These tokens are included in co-occurrence counts, encapsulating each document, for example: \"`<START>` All that glitters is not gold `<END>`\".\n",
        "\n",
        "The matrix rows (or columns) provide word vectors based on word-word co-occurrence, but they can be large. To reduce dimensionality, we employ Singular Value Decomposition (SVD), akin to PCA, selecting the top $k$ principal components. The SVD process decomposes the co-occurrence matrix $A$ into singular values in the diagonal $S$ matrix and new, shorter word vectors in $U_k$.\n",
        "\n",
        "This dimensionality reduction maintains semantic relationships; for instance, *doctor* and *hospital* will be closer than *doctor* and *dog*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x02bdeCCDT2q"
      },
      "outputs": [],
      "source": [
        "def distinct_words(corpus):\n",
        "    unique_words = set()\n",
        "\n",
        "    for document in corpus:\n",
        "        unique_words.update(document)\n",
        "\n",
        "    corpus_words = sorted(unique_words)\n",
        "    n_corpus_words = len(corpus_words)\n",
        "\n",
        "    return corpus_words, n_corpus_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7wyUsB8GbCk"
      },
      "source": [
        "▶*TODO* Complete `compute_co_occurrence_matrix` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gg3ix9GOZx"
      },
      "outputs": [],
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "\n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "\n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)):\n",
        "                Co-occurence matrix of word counts.\n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdK5u28TGqO_"
      },
      "outputs": [],
      "source": [
        "# Some sanity check (note that this is not an exhaustive test)\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2ind\n",
        "M_test_ans = np.array(\n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2ind\n",
        "assert (word2ind_ans == word2ind_test), \"Your word2ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2ind_ans, word2ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "# Test correct M values\n",
        "for w1 in word2ind_ans.keys():\n",
        "    idx1 = word2ind_ans[w1]\n",
        "    for w2 in word2ind_ans.keys():\n",
        "        idx2 = word2ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶*TODO* Create a function for reducing dimensionality with SVD"
      ],
      "metadata": {
        "id": "LroZSUn9_1qy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYbUtzOjHqQT"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def reduce_to_k_dim(M, n_components=2, n_iter=10):\n",
        "  # ------------------\n",
        "  # Write your implementation here.\n",
        "\n",
        "  # ------------------\n",
        "  return M_reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function for plotting words via M matrix"
      ],
      "metadata": {
        "id": "AI3ExwO1_l7R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKgs2kq1IScf"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "  print(words)\n",
        "  embeddings = [M_reduced[word2ind[word]] for word in words]\n",
        "  embeddings = np.array(embeddings)\n",
        "\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  for i, word in enumerate(words):\n",
        "      x, y = embeddings[i]\n",
        "      plt.scatter(x, y)\n",
        "      plt.text(x + 0.01, y + 0.01, word, fontsize=12)\n",
        "\n",
        "  plt.title('Word Embeddings')\n",
        "  plt.xlabel('Dimension 1')\n",
        "  plt.ylabel('Dimension 2')\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY7Ygj-jLir8"
      },
      "outputs": [],
      "source": [
        "def get_corpus(data):\n",
        "    return [[START_TOKEN] + [re.sub(r'[^\\w]', '', w.lower()) for w in f.split(\" \")] + [END_TOKEN] for f in data['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uBzeZZ5Im2b"
      },
      "outputs": [],
      "source": [
        "corpus = get_corpus(data)\n",
        "M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, n_components=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['hope', 'hopeful', 'hoping', 'funny', 'fun', 'smiles', 'happy', 'unhappy']\n",
        "\n",
        "plot_embeddings(M_normalized, word2ind_co_occurrence, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0nDYab7N0Oi"
      },
      "source": [
        "#### **Can we do it better?**\n",
        "\n",
        "\n",
        "It seems like some groups have occurred. 'fun', 'hope', and 'happy' are very close to each other and 'unhappy' is very far from them. Similarly 'smiles', 'hoping' and 'hopeful' are together, all these have positive feelings and might might be grouped together. That is a very good achievement!\n",
        "\n",
        "However, 'smiles' is closer to 'unhappy' than 'happy' or 'funny'. This is unexpected. Also 'hoping', 'hopeful', and 'hope' are not very close to each other.\n",
        "\n",
        "Can we improve these representations with neural networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x35CHRP6PNY_"
      },
      "source": [
        "### **3.1.3 GloVe**\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n",
        "The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence. Owing to the fact that the logarithm of a ratio equals the difference of logarithms, this objective associates (the logarithm of) ratios of co-occurrence probabilities with vector differences in the word vector space. Because these ratios can encode some form of meaning, this information gets encoded as vector differences as well. For this reason, the resulting word vectors perform very well on word analogy tasks\n",
        "\n",
        "See paper for the details: http://nlp.stanford.edu/pubs/glove.pdf\n",
        "\n",
        "\n",
        "  ![picture](https://drive.google.com/uc?export=view&id=1NzDecg8hkmGz-prcB3LrHVqGDjGMOGoB)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement it!\n",
        "\n",
        "To compute GloVe embeddings we will benefit from popular gensim library. Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. See: https://radimrehurek.com/gensim/. We alread instlled it at the beggining."
      ],
      "metadata": {
        "id": "T4GR0WIbWYyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-200\")"
      ],
      "metadata": {
        "id": "4-88UACOYkpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efVFl-ccPJaA"
      },
      "outputs": [],
      "source": [
        "def get_M_glove(model, words):\n",
        "  '''This function computes word vectors with glove model\n",
        "  and creates a matrix M by concatanation'''\n",
        "  word2ind = {word: i for i, word in enumerate(words)}\n",
        "  M_glove = np.stack([model.get_vector(w) for w in words])\n",
        "  return M_glove, word2ind\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M_glove, word2ind = get_M_glove(model, words)"
      ],
      "metadata": {
        "id": "CFmVoD6Yaik5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets map these embeddings to lower dim (again 2D)"
      ],
      "metadata": {
        "id": "-OoM11WPbh54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶*TODO* Reduce the dimension of `M_glove_reduced` embeddings to 2"
      ],
      "metadata": {
        "id": "5cnCou_R-ye1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# ------------------\n",
        "\n",
        "print(f\"\"\"The original size of embeedings for {len(words)} words is {M_glove.shape}.\n",
        "The reduced shape of embeddings is {M_glove_reduced.shape}. Succesfull!\"\"\")"
      ],
      "metadata": {
        "id": "qOroWpORbhBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶*TODO* Plot `M_glove_reduced` embeddings"
      ],
      "metadata": {
        "id": "66gmhE3vcSDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "YEezWqCjbXKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶*TODO* Compare the results of glove embeddings with the previous method. Explain what do you observe?\n",
        "\n",
        "> Your answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "g-Yg244qdJVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Sentence Embeddings**\n",
        "\n",
        "So far, we worked on word embeddings and succesfully integrated semantic information into low dimensional vectors. We can use these vectors already in our sentiment analysis but the task is still difficult because we need to analyze each word in sentences.\n",
        "\n",
        "An another approach is to create embeddings directly for sentences. There could be many models but we will use BERT model in this exercise.\n",
        "\n",
        "If you are not familiar with BERT, check out this blog: https://jalammar.github.io/illustrated-bert/"
      ],
      "metadata": {
        "id": "DhbrLz1seQuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets implement it. With Huggingface transformers, it is quite easy!"
      ],
      "metadata": {
        "id": "3XC8QInQiFPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "BJb4s_yUgWhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentence_embeddings(sentences):\n",
        "\n",
        "    inputs = tokenizer(sentences, return_tensors='pt',\n",
        "                       padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    sentence_embeddings = torch.mean(last_hidden_states, dim=1)\n",
        "    return sentence_embeddings"
      ],
      "metadata": {
        "id": "PJZkAsrGeQfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, simplify the task and create a dataset with only positive and negative samples"
      ],
      "metadata": {
        "id": "3CBqMC7pkCM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = 100\n",
        "dataset_counter = 0\n",
        "dataset, labelset = [], []\n",
        "\n",
        "idx = 0\n",
        "while dataset_counter < dataset_size:\n",
        "  sample = sentiment_dataset[\"train\"][idx]\n",
        "\n",
        "  if sample['label'] == 0 or sample['label'] == 2:\n",
        "    dataset.append(sample['text'])\n",
        "    labelset.append(sample['label'])\n",
        "    dataset_counter += 1\n",
        "\n",
        "  idx += 1"
      ],
      "metadata": {
        "id": "BCsNsjUKkJAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Dataset size: {len(dataset)}/{len(labelset)}')"
      ],
      "metadata": {
        "id": "EWjObleflKtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create embeddings for sentences"
      ],
      "metadata": {
        "id": "7573axbvJ3Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = create_sentence_embeddings(dataset)\n",
        "print(f'The shape of embeddings are {embeddings.shape}.')"
      ],
      "metadata": {
        "id": "l9jScvfJgZPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create reference sentences"
      ],
      "metadata": {
        "id": "DeJvgJr6CvSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reference = [\"That is very positive\"]\n",
        "negative_reference = [\"That is very negative\"]"
      ],
      "metadata": {
        "id": "JFqdjPCgCvBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings for references"
      ],
      "metadata": {
        "id": "dutvUrecDwCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_positive = create_sentence_embeddings(positive_reference)\n",
        "embeddings_negative = create_sentence_embeddings(negative_reference)\n",
        "print(f'The shape of positive embeddings are {embeddings_positive.shape}.')\n",
        "print(f'The shape of negative embeddings are {embeddings_negative.shape}.')"
      ],
      "metadata": {
        "id": "c0KVTjUeDy3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use cosine similarity, and decide for each sentence, if they are close to positive reference sentence or negative"
      ],
      "metadata": {
        "id": "7NAhSLwXD_U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_positive = cosine_similarity(embeddings_positive, embeddings)\n",
        "similarity_negative = cosine_similarity(embeddings_negative, embeddings)\n",
        "\n",
        "similarity_positive = np.squeeze(similarity_positive)\n",
        "similarity_negative = np.squeeze(similarity_negative)"
      ],
      "metadata": {
        "id": "xIxhvJsXEHFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, make predictions. If a sentence is closer to negative reference, we classify it as negative. And if a sentence is closer to positive reference, we classify it as positive."
      ],
      "metadata": {
        "id": "sXzL_6EbKTt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.zeros((len(labelset),))\n",
        "predictions[similarity_positive > similarity_negative] = 2"
      ],
      "metadata": {
        "id": "qxfC6T_YKuDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute accuracy and see the performance"
      ],
      "metadata": {
        "id": "CvmPfr1kLD_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(labelset, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "95QWmdiMLHuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already achieved slightly above the chance level without any training. We can improve it by finetunning the model with our data, using more powerful models, or better reference sentences.  "
      ],
      "metadata": {
        "id": "GWr1iA-RWmlj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcebCwXRuCPCOwQ4zBKlbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}